\chapter{Modelos de Redes Complexas}
\label{chapter:networks}

Grafos são estruturas de dados que codificam relações e que estão presentes em
uma grande variedade de cenários.
São compostos por nós que representam elementos, quaisquer que sejam, e arestas
que são as relações entre os elementos.
Esses elementos podem ser de um único tipo, formando grafos homogêneos, como por
exemplo uma rede de amizades em que todos elementos são pessoas.
Os grafos heterogêneos são os que possuem vértices de mais de um tipo, neste
caso, um exemplo é grafo de cinema no qual tem como vértices tanto filmes quanto
atores.
Em relação as arestas, quando estas representam uma ligação de sentido único são
formados grafos direcionais.
Um grafo de empréstimos financeiras entre um grupo de pessoas será um grafo
direcional, enquanto a rede de pessoas que se abraçaram será não-direcionada.
As arestas também podem ter pesos distintos.
Podemos categorizar o grafo como estático caso ele não sofra alterações,
ou dinâmico, como os grafos que evoluem com o passar do tempo.

Tratando-se de redes sociais, grafos são especialmente importantes, dado que
a relação entre os usuários é a principal componente desses serviços.
Portanto, ao estudar as redes sociais é essencial entender essas relações.
Há diversas maneiras de representar essas redes, sendo a mais comum formando um
grafo homogêneo em que cada usuário é um vértice e as arestas codificando as
interações entre os mesmos.

Há quatro principais categorias de modelos de redes: categorização de nós, predição
de arestas, categorização do grafo e detecção de comunidade.
A categorização de nós é a tarefa que classifica cada nó em uma ou mais classes.
Um exemplo comum na literatura é a classificação de artigos acadêmicos em áreas
de conhecimento~\cite{sen08}.
A predição de arestas consiste em dado um par de vértices determinar a
probabilidade de existir uma aresta entre eles. Uma aplicação dessa tarefa no
contexto de redes sociais é a recomendação de conexão entre usuários.
A detecção de comunidade consiste no agrupamento de nós de uma rede,
identificando os conjuntos de nós que tem maior conexão entre si.
Em alguns circunstancias o que se tem interesse é entender alguma propriedade
formada pelo grafo inteiro.
Nesses casos a categorização de grafos classifica a rede como um todo em uma ou
mais classes.
Este método pode ser usado, por exemplo, na classificação de função de
proteínas a partir de sua estrutura molecular~\cite{shervashidze11}.
Além dessas quatro categorias, a literatura de redes complexas apresenta outras
variedades de modelos como o de propagação de epidemias em redes, modelagem de
evolução de grafos dinâmicos, entre outros.

Devido ao foco desse trabalho na representação de usuários de redes sociais,
serão estudadas nesse capítulo técnicas de representações de vértices.
Assim como no caso de documentos textuais, esses modelos geralmente visam a
codificar a informação de um nó em um vetor de baixa dimensionalidade,
capturando tanto seus atributos individuais, quanto os decorrentes da estrutura
de conexões do mesmo.
Desta forma, essa informação poderá ser combinada com a representação das
mensagens e assim treinar um classificador que considere ambas as partes para
realização da tarefa desejada.

% FIGURA: com subfiguras representado cada uma das tarefas citadas
% - classificacao de nós: grafos com nós em 2 cores
% - classificação de grafos: multiplos grafos, divididos em 2 cores...

% pegar exemplo de figura de artigo de classificacao de nos
% de 2 grafos egocentricos de classes distintas

% grande referencia em graph embeddings
% A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications
% Graph Embedding Techniques, Applications, and Performance: A Survey

% Representation Learning on Graphs: Methods and Applications
% Deep Learning on Graphs: A Survey

\section{Modelos Baseados em Fatoração Matricial}

Uma das forma mais comuns de descrever um grafo é por suas matrizes, sendo a
principal delas a matriz de adjacência.
A matriz de adjacência $A$ é uma matriz quadrada de dimensionalidade $n \times n$,
sendo $n$ o número de vértices, na qual o elemento $a_{ij}$ tem valor $1$ caso o
vértice $v_i$ seja conectado ao $v_j$ e $0$ caso contrario.
Além da matriz de adjacência, há outras matrizes extraídas dos grafos, como
a matriz Laplaciana e a matriz de centralidade de Katz.

Essas matrizes podem ser exploradas para extração de informação das redes.
Os modelos de representação de vértices por fatoração matricial foram os
primeiros métodos desenvolvidos.
Nos casos em que a matriz é positiva e semi-definida, como a matriz Laplaciana, é
possível aplicar a decomposição em autovalores para obter uma representação.
Nos outros casos, em geral, são utilizados métodos iterativos visando a
minimizar uma função custo relacionada à qualidade da representação obtida~\cite{goyal18}.
Serão explicados em seguida um conjunto de técnicas baseados na fatoração de matrizes.

\subsubsection{Locally Linear Embedding (LLE)}

Um dos modelos de representação mais simples é o desenvolvido por \citet{roweis00}.
Esse algoritmo considera a premissa de que um nó é uma combinação linear de
seus vizinhos.
Portanto, tendo a matriz de adjacência $W_{ij}$ e o vetor de representação de um
nó como $Y_i$, definimos o mesmo como:

\begin{equation}
    Y_i \approx \sum_j{W_{ij}Y_j}
\end{equation}

Assim, função custo a ser minimizada é a que minimiza o erro de representação
do somatório de todos os nós, de forma que a função custo $\phi(Y)$
é dada por:

\begin{equation}
    \phi(Y) = \sum_i{|Y_i - \sum_j{W_{ij}Y_j}|^2}
\end{equation}

Sendo necessário respeitar a restrição $\frac{1}{N}Y^TY = 1$ para remover as
soluções degeneradas.
Essa técnica obtém uma representação de nós que preserva as distância de
primeira ordem da rede, ou seja, vizinhos diretos no grafo original vão obter
representações próximas.

\subsubsection{Automapas Laplacianos}

Esta técnica, desenvolvida por \citet{belkin02}, também tem como objetivo obter
representações próximas entre vizinhos.
No entanto, desta vez é inspirada na equação de fluxo de calor, na qual a transmissão de
energia decai quadraticamente com a distância entre os corpos.
Para isso, é utilizada a matriz laplaciana do grafo.
Sua função custo é dada pela Equação~\ref{eq:laplacian_eigenmaps}, na qual $L$ é
a laplaciana do grafo.

\begin{equation} \label{eq:laplacian_eigenmaps}
\begin{aligned}
    \phi(Y) &= \frac{1}{2} \sum_{ij}{|Y_i - Y_j|^2 W_{ij}} \\
            &= tr(Y^TLY)
\end{aligned}
\end{equation}

\subsubsection{HOPE}

Os algoritmos apresentados anteriormente são efetivos para manter a distância
entre vizinhos de um grafo não-direcionado.
Entretanto, grafos direcionados reais têm, em grande parte das vezes, sua
distribuição de grau obedecendo a uma lei de potência, como é o caso dos grafos
formados em redes sociais.
Não apresentam, assim, propriedades simétricas entre pares de vértices.
Para obter representações que respeitem essa assimetria \citet{ou16},
desenvolveram o algoritmo \textit{High-Order Proximity preserved Embedding}
(HOPE).
A chave deste algoritmo é utilizar métricas de proximidade de ordem superior em
sua função custo, preservando assim as propriedades da assimetria.
Como qualquer medida de proximidade pode ser aplicada nesse algoritmo, \citet{ou16}
testaram medidas como a proximidade de Katz e Pagerank personalizado, mostrando
que em vários casos essas medidas possuem aproximações que diminuem o custo
computacional do algoritmo.

HOPE representa cada vértice em 2 vetores, um de entrada e um de saída, que
denominaremos respectivamente $Y^s$ e $Y^t$.
A função custo é dada pela Equação~\ref{eq:hope}, na qual $S$ representa a matriz
de proximidade entre os nós, dada por qualquer uma das medidas escolhidas.
O termino da otimização resulta no par de vetores, de entrada e saída, que
representam cada um dos nós da rede.

\begin{equation} \label{eq:hope}
    \phi = \vert\vert S - Y^s {Y^t}^{T} \vert\vert^2_F
\end{equation}

\section{Modelos Baseados em Passeios Aleatórios}

Outras estratégias amplamente adotada são as baseadas em Passeios Aleatórios.
O Passeio Aleatório é um processo estocástico que consiste em selecionar um
vértice inicial no grafo e completar uma sequência de passos aleatórios pelos
seus vizinhos.
Como o resultado do Passeio Aleatório é uma sequência de vértices, podemos
utilizá-lo como uma função de proximidade entre os nós.
Com essa finalidade costuma-se realizar múltiplos Passeios Aleatórios de tamanho
fixo partindo de cada vértice da rede, formando um conjunto de realizações.

Por ser um método iterativo e não necessitar o cálculo de nenhuma matriz completa
do grafo, esse método apresenta vantagem em custo operacional para redes
grandes, dado que de computação dessas matrizes tem custo quadrático em memória.
Além disso, a iteratividade do método também permite que novos elementos sejam
adicionados ao grafo sem requerer um retreinamento completo do modelo,
característica essencial para seleção de algoritmos em diversas aplicações.
A seguir serão apresentados os principais modelos deste grupo.

\subsubsection{Deepwalk}

Deepwalk~\cite{perozzi14} se inspira em modelos de representações de palavras,
utilizando como entrada sequências de iterações de Passeios Aleatórios truncadas
em poucos passos.
Os autores observaram que, assim como a distribuição de ocorrência de palavras de
uma língua segue uma lei de potência, o mesmo comportamento é observado em boa
parte das redes formadas no mundo real, como a de conexão entre pessoas.
Desta forma, eles adaptaram algoritmos previamente aplicados em predição de
palavras para obter as representações dos vértices.

O modelo proposto é muito similar à variação \textit{skipgram} do
Word2Vec~\ref{sec:w2v}: os passeios aleatórios truncados formam sequências de
tamanho $2k + 1$, e o treinamento do algoritmo é feito de maneira a maximizar a
probabilidade de predição do contexto a partir do vértice na posição central do
passeio.
A Equação~\ref{eq:deepwalk} demonstra a função citada.
Dado que $v_i$ é o vértice central de um certo passeio e $\Phi$ é o mapeamento
de um vetor em sua representação, que queremos encontrar, então:

\begin{equation} \label{eq:deepwalk}
    \operatorname{max} P(\{v_{i-k},...,v_{i-1},v_{i+1},...,v_{i+k}\}|\Phi(v_i))
\end{equation}

O processo é feito de forma iterativa por conta de cada realização do Passeio
Aleatório compor uma entrada do algoritmo.
A otimização é dada por gradiente descendente da função custo $\phi$ que maximiza
a probabilidade Equação~\ref{eq:deepwalk}, que é apresentada na
Equação~\ref{eq:deepwalk_cost}.
Outras adaptações propostas posteriormente ao Word2Vec para diminuir o custo
computacional de treinamento também podem ser adotadas no Deepwalk, como
\textit{softmax} hierárquico e \textit{negative sampling}.

\begin{equation} \label{eq:deepwalk_cost}
    \phi = -\log P(\{v_{i-k},...,v_{i-1},v_{i+1},...,v_{i+k}\}|\Phi(v_i))
\end{equation}

\subsubsection{Node2Vec}

O Node2Vec~\cite{grover16} também é um algoritmo inspirado no Word2Vec.
Seu processo de otimização é idêntico ao apresentado pelo Deepwalk, também
aplicado sobre sequências de passeios aleatórios de tamanho fixo.
A diferença entre ambos está na estratégia de passeio aleatório, que além de
capturar a vizinhança de um dado vértice, visa também a capturar a função do
mesmo na estrutura ao seu redor.
Enquanto a vizinhança de um nó compõe uma parte de sua informação, o
posicionamento do mesmo em sua comunidade também é relevante.
Dentro de uma rede, um nó central de uma comunidade pode apresentar
características mais semelhantes ao ponto central de outra comunidade
do que a algum vértice vizinho a ele que tenha poucas conexões.

A proposta desenvolvida por \citet{grover16} consiste em um Passeio Aleatório
enviesado que parametriza quão relevante é cada um desses dois comportamentos.
Ao se escolher parâmetros que priorizem a vizinhança do vértice, a amostragem do
Passeio Aleatório enviesado passa a se assemelhar a uma busca em profundidade.
Por outro lado, focando-se na estrutura formada pelas conexões do nó, obtemos uma
amostragem semelhante à busca em largura.

No Passeio Aleatório não enviesado, a probabilidade de transição $\pi_{uv}$ do
nó $u$ para o nó $v$ é dada por $\pi_{uv} = \frac{w_{uv}}{\sum{w_u}}$, no qual
$w_{uv}$ é o peso da aresta entre $u$ e $v$ e $\sum{w_u}$ é o somatório dos
pesos das arestas de $u$, neste caso operando como regularizador da probabilidade.
Para atingir seu objetivo, Node2Vec multiplica a probabilidade de transição
original do Passeio Aleatório por um fator $\alpha(s, v)$ parametrizado pelos
parâmetros $p$ e $q$, como mostra a Equação~\ref{eq:node2vec_alpha}.
Nela, $s$ representa o vértice em que o Passeio Aleatório amostrou no passo
anterior ao passo de posição $u$, enquanto $d_{sv}$ representa a distância
mínima entre o par de vértices $s$ e $v$.
Esses parâmetros controlam quão rápido o Passeio Aleatório explora a rede.

\begin{equation} \label{eq:node2vec_alpha}
    \alpha(s, v) =
    \begin{cases}
        \frac{1}{p} ,& \text{se } d_{sv} = 0\\
        1           ,& \text{se } d_{sv} = 1\\
        \frac{1}{q} ,& \text{se } d_{sv} = 2
    \end{cases}
\end{equation}

O parâmetro $p$ é chamado de parâmetro de retorno.
A escolha de valores altos para esse fator faz com que o Passeio Aleatório evite
re-amostrar vértices recém amostrados, enquanto valores baixos forçam o
algoritmo a manter a amostragem localizada em uma distância pequena de seu ponto
de partida.
Por sua vez, o parâmetro $q$ regula a probabilidade de se amostrar nós distantes
do vértice de partida.
Valores baixos de $q$ resultam em uma amostragem semelhante à busca em
profundidade enquanto valores altos levam a um padrão de busca em largura.
Resumidamente, a probabilidade de transição do passeio aleatório proposto por
Node2Vec é dada pela Equação~\ref{eq:node2vec_pi}.

\begin{equation} \label{eq:node2vec_pi}
    \pi(s, u, v) = \frac{w_{uv} \alpha(s,v)}{\sum_{x} w_{ux} \alpha(s,x)}
\end{equation}

Os autores mostram que a adoção dessa estratégia de amostragem resultou em
ganhos significativos de performance do algoritmo para tarefas de classificação
de vértices.
Apesar da maior complexidade, o algoritmo mantém o aumento linear do custo
computacional com relação ao número de nós da rede, possibilitando sua aplicação
em redes de grande volume.

\section{Redes Convolucionais de Grafos}

O sucesso de \textit{Deep Learning} em outras áreas de estudo, como em
processamento de linguagem natural, também impactou o estudo de redes complexas.
Ao longo da última década observamos adaptações desses modelos para serem usados
em grafos, como por exemplo os \textit{autoencodes}~\cite{wang16}\cite{cao16}
e redes recorrentes~\cite{scarselli08}\cite{you18}.
Dentre eles as variações de redes convolucionais aplicadas a grafos se tornaram
o principais algoritmos deste grupo, superando em performance os algoritmos mais
utilizados a época em varias tarefas.
Assim como uma rede convolucional convencional, o treinamento dessas redes é
supervisionado, feito por \textit{back-propagation} de uma função custo especifica
da tarefa.

Para tal, se faz necessária uma adaptação do operador de convolução para aplicação
em grafos. Estas adaptações são divididos em dois tipos~\cite{zhang20}: convoluções
espectrais~\cite{bruna13}\cite{defferrard16} e convoluções espaciais~\cite{kipf16}.
A convolução espacial desenvolvida por \citet{kipf16}, chamada rede
convolucional de grafos (GCN) se tornou uma das mais populares.
A Equação~\ref{eq:gcn_layer} define a camada de convolução usada nesse
algoritmo.

\begin{equation} \label{eq:gcn_layer}
    H^{(l+1)} = \sigma (\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^l W^l)
\end{equation}

A matriz $\tilde{A} = A + I$ é a matriz de adjacência somada à matriz
identidade, correspondente a um grafo com um laço \footnote{aresta conectando um
vértice a ele mesmo} adicionado a cada vértice.
A matriz diagonal $\tilde{D}$ é composta pela diagonal
$\tilde{D}_{ii} = \sum_{j}\tilde{A}_{ij}$, sendo que $H^l$ representa a saída da
camada $l$ e $W^l$ representa os pesos treináveis da mesma.
Por fim, temos a função de ativação escolhida $\sigma$.
Os autores mostram em seu trabalho a relação desta camada com a aproximação de
primeira ordem da convolução espectral do grafo.

Apesar do GCN não ser especificamente voltado para obtenção de representações,
os filtros convolucionais obtidos após o treinamento podem ser utilizados como
uma.
Uma dificuldade deste modelo é que, assim como os modelos de fatoração matricial,
só é possível aplicar GCN em grafos fixos, pois ao precisar adicionar novos vértices,
precisa-se retreinar o modelo.
Dentre os algoritmos que se propõem a atacar esse problema, destacamos o
GraphSAGE~\cite{hamilton17}, que obtém a representação de um vértice a partir
de uma função de agregação de seus vizinhos, conseguindo por sua vez incorporar
novos nós mesmo que estes não tenham participado do treinamento.

Por se tratar de um método treinado supervisionado este vai contra o
objetivo deste trabalho de reduzir a necessidade de anotação de dados.
Porém, \citet{velickovic18} desenvolveram um método, chamado Graph Infomax, que
visa permitir o treinamento não supervisionado mesmo para modelos originalmente
supervisionados.
Este tem como objetivo maximizar a informação mútua entre representações globais e
locais do grafo, codificando assim informação referente tanto a rede inteira
quanto a vizinhança de cada vértice.
