\chapter{Processamento de Linguagem Natural}
\label{chapter:nlp}

Neste capitulo serão apresentadas as técnicas de Processamento de Linguagem
Natural que compõe um classificador, como o de análise de sentimento.
De modo geral, esse processo é composto de 3 etapas, como demostrado no diagrama
(TODO).
As seções a seguir descrevem cada uma destas fases.
% TODO: diagrama de etapas de um classificador por NLP

\section{Pré-Processamentos}

A primeira etapa aplicada para elaboração de modelos de NLP é o
pré-processamento.
Esta fase consiste na limpeza e preparação dos dados, visando melhorar a
performance do classificador seja retirando ruídos dos textos, reduzindo o
tamanho do vocabulário ou formatando o texto de maneira a facilitar a modelagem.
O volume do vocabulário considerado é costuma ser limitado seja pelos recursos
computacionais quanto pelo requisito mínimo de estatística das palavras na base
de dados.
Portanto, técnicas de pré-processamento que reduzam o tamanho total do
vocabulário tem um importante papel na garantia de bom funcionamento dos
classificadores.

Como a maior parte dos modelos de NLP trabalham a nível de palavra é necessário
separar separar o documento em frases, com algoritmos como o Punkt~\cite{kiss06},
e posteriormente, em palavras.
Esse processo chamado \textit{tokenização} precisa ser robustos a abreviações,
números e características do idioma ao qual será aplicado, como contração de
palavras.
Se tratando de redes sociais, também é relevante tratar os links, as
\textit{hashtags} e as menções a usuários.

Algoritmos de correção ortográfica~\cite{damerau64}\cite{navarro01} podem ser
eficientes para aprimorar a qualidade dos textos, principalmente se tratando de
meios de comunicação dinâmicos como as redes sociais.

Técnicas de stemização consistem na extração do o radical das palavras,
como o obtido pelo algoritmo de Porter~\cite{porter80}, um exemplo é dado com a
palavra "montanha" que possuí radical "mont", o mesmo obtido pela palavra
"monte".
Por outro lado, o processos de lematização tem finalidade parecida, porém
transforma a palavra em sua forma base, forma como elas aparecem no dicionario,
podendo então diferenciar palavras com o mesmo radical, como "banco" e "bancários".
Ambas as técnicas visam tornar as etapas posteriores menos sensíveis a flexões
gramaticais, além de colaborar na redução do vocabulário.

Uma das principais etapas do pré-processamento é a remoção das
\textit{stopwords}, conjunto de palavras que informação pouco discriminante
para uma dada aplicação~\cite{lo05}, geralmente são compostas pelas palavras
mais comuns da língua, principalmente artigos e preposições.
O objetivo da remoção das \textit{stopwords} é diminuir ruídos dos dados
textuais, assim simplificando a etapa de modelagem.
\citet{saif14} fizeram um estudo comparando diversos métodos de seleção de
\textit{stopwords} e o impacto das mesmas na classificação de sentimento de
\textit{tweets}.
A identificação de classe gramatical, em inglês \textit{part of speech}, além de
ser tipicamente utilizada como entrada de modelos de NLP, também pode ser útil
para selecionar \textit{stopwords}.

\section{Representações}

Uma vez que os tratamentos iniciais dos textos são feitos chega-se a etapa de
preparar esses dados para serem processados pelo modelo.
Para isso, os documentos são transformados de sequências de palavras em vetores
ou matrizes.
Há diversas técnicas desenvolvidas com essa finalidade, estas podem ser
divididas em representações esparsas e representações densas.
As representações esparsas são as mais simples de serem aplicadas dado que, em
geral, não dependem do treinamento de nenhum modelo.
Entretanto, as representações esparsas resultam em vetores ou matrizes de
dimensão na ordem de, pelo menos, o tamanho do vocabulário escolhido, como os
vocabulários costumam ser muito extensos (centenas de milhares de palavras em
alguns casos) o tamanho e a esparsidade da representação obtida podem dificultar
o treinamento dos classificadores.
Para contornar essa dificuldade foram criados algoritmos de representações
densas.
Por transformarem os documentos em vetores ou matrizes de baixa dimensão estes
algoritmos foram os responsáveis pela viabilidade de utilização de técnicas de
\textit{Deep Learning} aplicadas ao processamento de linguagem natural.
Nessa seção descreveremos algumas das principais técnicas de representação de
texto.

\subsection{Codificação One-Hot}

A codificação \textit{One-Hot} representa cada palavra de maneira maximamente
esparsa.
Para tal, é definido um espaço vetorial em que cada palavra do vocabulário
utilizado é equivalente a uma dimensão do espaço.
Portanto, um documento pode ser transcrito dessa forma em uma sequência de
vetores, ou matriz, em que cada palavra é um vetor com valor unitário na
dimensão da própria palavra e zero nas outras, como mostra a figura TODO.

% TODO: figura mostrando exemplo

Frequentemente encontramos nas línguas palavras compostas ou expressões.
Essas informações são perdidas na codificação \textit{One-Hot}.
Uma forma de se atenuar esse problema são com os chamados \textit{n-gramas}.
A ideia do \textit{n-grama} é formar tokens de 2, 3, ou \textit{n} palavras e
utiliza este conjunto de palavras como dimensão do espaço.
Entretanto, o aumento no número de palavras por token também gera um aumento
significativo do número de dimensões, dificultando o treinamento do
classificador.

\subsection{Bag-of-Words}

A codificação \textit{Bag-of-Words} é uma alternativa a representação de
mensagens como matrizes compostas de vetores \textit{One-Hot} de suas palavras.
Esta é feita pela soma destes mesmos vetores~\cite{manning10}.
Portanto, a representação final é dada por um único vetor, de tamanho
correspondente ao do vocabulário utilizado.
Esta técnica também tem a vantagem de transformar documentos de tamanho variados
em vetores de mesmas dimensões, fator que precisa ser contornado em codificações
baseadas em palavras.

% TODO: figura bag-of-words

Entretanto, visto que a distribuição de palavras em um corpus, em geral, segue a
lei de Zipf~\cite{powers98}, ou seja, sua frequência segue uma distribuição em
lei de potência.
Neste caso mesmo retirando \textit{stopwords}, as palavras mais comuns do
vocabulário ainda dominarão os documentos, e este comportamento pode ser
prejudicial para o treinamento dos modelos que serão menos expostos a palavras
incomuns.

% TODO: exemplo zipf grafico ou equacao

Para atenuar esse problema pode-se aplicar o \textit{term frequence-inverse
document frequence} (TF-IDF)~\cite{salton88}.
Neste caso, a representação segue a mesma estrutura proposta pela codificação
\textit{bag-of-words}, entretanto, cada palavra é ponderada por um multiplicador
inversamente proporcional a sua frequência nos documentos.

% TODO: equação tf-idf

\textit{Bag-of-words} e TF-IDF são métodos de representação muito presentes na
literatura pela sua simplicidade de implementação e pelos benefícios
anteriormente descritos, em especial em conjunto com modelos como a SVM (TODO
link secao) que apresenta menos dificuldades de ser treinada em dados esparsos.
Entretanto, um componente fundamental da linguagem é perdido nesse processo, o
contexto em que se insere cada palavra.
Pela representação agrupar todos os tokens em um único vetos a ordem das
palavras no documento é perdida, o que em muitos casos pode corresponder na
inviabilidade de uma classificação precisa do mesmo.

% TODO: talvez mostrar exemplo em que a ordem faria diferença

As técnicas de representações densas que serão apresentadas a seguir além de
resolverem o obstáculo da dimensionalidade dos dados também viabilizam a
classificação por modelos que levem em conta o contexto de cada palavra.

\subsection{Word2Vec}

\textit{Word2Vec}~\cite{mikolov13} foi uma das primeiras técnicas de
representação densa de palavras amplamente adota pela indústria e academia.
Representações densas são aquelas em que cada palavra é transformada em um vetor
de números reais de baixa dimensionalidade, tipicamente dezenas ou poucas
centenas de dimensões.
\citet{mikolov13} mostraram que essa representação é capaz de capturar parte dos
sentidos semânticos e sintáticos das palavras, aproximando as que são sinônimas
ou exerçam a mesma função gramatical, atenuando assim o problema da disparidade
de frequência das palavras.

Esta técnica é um modelo constituído de uma rede neural de uma camada escondida.
Para o treinamento do mesmo são feitas janelas de um número arbitrário de
palavras que servirão como entrada do modelo.
Dessas janelas há duas variações: o \textit{continuous bag-of-words} em que
o modelo é treinado para prever a palavra central da janela de entrada a partir
das outras palavras da janela, ou o \textit{skipgram} que a partir da palavra
central da treina-se para prever o seu contexto.
As entradas e saídas do modelo são representadas pela codificação
\textit{one-hot} dos termos.
A quantidade de neurônios escolhido para a camada escondida da rede resultará na
dimensionalidade da representação.
A representação das palavras serão os pesos da camada de entrada do modelo
treinado.

% TODO diagramas

Apesar do \textit{Word2Vec} ser um modelo que precisa ser treinando, esse
treinamento é não-supervisionado dado que tanto as entradas quanto saídas do
modelo são obtidas diretamente dos documentos.
Desta forma, foi possível aplicar o algoritmo a grandes bases de dados
mineiradas da internet.
Esse alto volume de dados de treinamento foi essencial para que bons resultados
fossem alcançados.

Por ser a primeira representação densa de sucesso, o \textit{Word2Vec} foi
essencial para a aplicação de classificadores também baseados em redes neurais,
como os de \textit{Deep Learning} que obtiveram grande êxito em diversas
tarefas de processamento de linguagem natural.
Outras técnicas semelhantes e também amplamente adotadas na indústria foram
desenvolvidas neste mesmo período de tempo como o GloVe~\cite{pennington14} e
o FastText~\cite{bojanowski17}.

\subsection{Seq2Seq (BERT)} \label{sec:seq2seq}
% TODO titulo

O sucesso do \textit{Word2Vec}, baseado em redes neurais \textit{feed forward}
inspirou a experimentação de outros tipos de redes neurais, dentre as quais as
redes neurais recorrentes e suas variações obtiveram bons resultados na
representação de palavras.
Nesta seção serão descritos alguns destes algoritmos.

\section{Classificadores}
\subsection{Baseados em Dicionário} \label{sec:dictionary}
\subsection{Lineares}
\subsection{Não Lineares}
\subsection{Deep Learning}
% diferença principal de deep p tradicional é: contexto
% inspiração: http://dataskeptic.com/blog/episodes/2019/natural-language-processing

% falar de diferentes dificultades de NLP e citar trabalhos relacionados no cap 3
