\chapter{Processamento de Linguagem Natural}
\label{chapter:nlp}

Neste capitulo serão apresentadas as técnicas de Processamento de Linguagem
Natural que compõe um classificador, como o de análise de sentimento.
De modo geral, esse processo é composto de 3 etapas, como demostrado no diagrama
(TODO).
As seções a seguir descrevem cada uma destas fases.
% TODO: diagrama de etapas de um classificador por NLP

\section{Pré-Processamentos}

A primeira etapa aplicada para elaboração de modelos de NLP é o
pré-processamento.
Esta fase consiste na limpeza e preparação dos dados, visando melhorar a
performance do classificador seja retirando ruídos dos textos, reduzindo o
tamanho do vocabulário ou formatando o texto de maneira a facilitar a modelagem.
O volume do vocabulário considerado é costuma ser limitado seja pelos recursos
computacionais quanto pelo requisito mínimo de estatística das palavras na base
de dados.
Portanto, técnicas de pré-processamento que reduzam o tamanho total do
vocabulário tem um importante papel na garantia de bom funcionamento dos
classificadores.

Como a maior parte dos modelos de NLP trabalham a nível de palavra é necessário
separar separar o documento em frases, com algoritmos como o Punkt~\cite{kiss06},
e posteriormente, em palavras.
Esse processo chamado \textit{tokenização} precisa ser robustos a abreviações,
números e características do idioma ao qual será aplicado, como contração de
palavras.
Se tratando de redes sociais, também é relevante tratar os links, as
\textit{hashtags} e as menções a usuários.

Algoritmos de correção ortográfica~\cite{damerau64}\cite{navarro01} podem ser
eficientes para aprimorar a qualidade dos textos, principalmente se tratando de
meios de comunicação dinâmicos como as redes sociais.

Técnicas de stemização consistem na extração do o radical das palavras,
como o obtido pelo algoritmo de Porter~\cite{porter80}, um exemplo é dado com a
palavra "montanha" que possuí radical "mont", o mesmo obtido pela palavra
"monte".
Por outro lado, o processos de lematização tem finalidade parecida, porém
transforma a palavra em sua forma base, forma como elas aparecem no dicionario,
podendo então diferenciar palavras com o mesmo radical, como "banco" e "bancários".
Ambas as técnicas visam tornar as etapas posteriores menos sensíveis a flexões
gramaticais, além de colaborar na redução do vocabulário.

Uma das principais etapas do pré-processamento é a remoção das
\textit{stopwords}, conjunto de palavras que informação pouco discriminante
para uma dada aplicação~\cite{lo05}, geralmente são compostas pelas palavras
mais comuns da língua, principalmente artigos e preposições.
O objetivo da remoção das \textit{stopwords} é diminuir ruídos dos dados
textuais, assim simplificando a etapa de modelagem.
\citet{saif14} fizeram um estudo comparando diversos métodos de seleção de
\textit{stopwords} e o impacto das mesmas na classificação de sentimento de
\textit{tweets}.
A identificação de classe gramatical, em inglês \textit{part of speech}, além de
ser tipicamente utilizada como entrada de modelos de NLP, também pode ser útil
para selecionar \textit{stopwords}.

\section{Representações}

Uma vez que os tratamentos iniciais dos textos são feitos chega-se a etapa de
preparar esses dados para serem processados pelo modelo.
Para isso, os documentos são transformados de sequências de palavras em vetores
ou matrizes.
Há diversas técnicas desenvolvidas com essa finalidade, estas podem ser
divididas em representações esparsas e representações densas.
As representações esparsas são as mais simples de serem aplicadas dado que, em
geral, não dependem do treinamento de nenhum modelo.
Entretanto, as representações esparsas resultam em vetores ou matrizes de
dimensão na ordem de, pelo menos, o tamanho do vocabulário escolhido, como os
vocabulários costumam ser muito extensos (centenas de milhares de palavras em
alguns casos) o tamanho e a esparsidade da representação obtida podem dificultar
o treinamento dos classificadores.
Para contornar essa dificuldade foram criados algoritmos de representações
densas.
Por transformarem os documentos em vetores ou matrizes de baixa dimensão estes
algoritmos foram os responsáveis pela viabilidade de utilização de técnicas de
\textit{Deep Learning} aplicadas ao processamento de linguagem natural.
Nessa seção descreveremos algumas das principais técnicas de representação de
texto.

\subsection{Codificação One-Hot}

A codificação \textit{One-Hot} representa cada palavra de maneira maximamente
esparsa.
Para tal, é definido um espaço vetorial em que cada palavra do vocabulário
utilizado é equivalente a uma dimensão do espaço.
Portanto, um documento pode ser transcrito dessa forma em uma sequência de
vetores, ou matriz, em que cada palavra é um vetor com valor unitário na
dimensão da própria palavra e zero nas outras, como mostra a figura TODO.

% TODO: figura mostrando exemplo

Frequentemente encontramos nas línguas palavras compostas ou expressões.
Essas informações são perdidas na codificação \textit{One-Hot}.
Uma forma de se atenuar esse problema são com os chamados \textit{n-gramas}.
A ideia do \textit{n-grama} é formar tokens de 2, 3, ou \textit{n} palavras e
utiliza este conjunto de palavras como dimensão do espaço.
Entretanto, o aumento no número de palavras por token também gera um aumento
significativo do número de dimensões, dificultando o treinamento do
classificador.

\subsection{Bag-of-Words}

A codificação \textit{Bag-of-Words} é uma alternativa a representação de
mensagens como matrizes compostas de vetores \textit{One-Hot} de suas palavras.
Esta é feita pela soma destes mesmos vetores~\cite{manning10}.
Portanto, a representação final é dada por um único vetor, de tamanho
correspondente ao do vocabulário utilizado.
Esta técnica também tem a vantagem de transformar documentos de tamanho variados
em vetores de mesmas dimensões, fator que precisa ser contornado em codificações
baseadas em palavras.

% TODO: figura bag-of-words

Entretanto, visto que a distribuição de palavras em um corpus, em geral, segue a
lei de Zipf~\cite{powers98}, ou seja, sua frequência segue uma distribuição em
lei de potência.
Neste caso mesmo retirando \textit{stopwords}, as palavras mais comuns do
vocabulário ainda dominarão os documentos, e este comportamento pode ser
prejudicial para o treinamento dos modelos que serão menos expostos a palavras
incomuns.

% TODO: exemplo zipf grafico ou equacao

Para atenuar esse problema pode-se aplicar o \textit{term frequence-inverse
document frequence} (TF-IDF)~\cite{salton88}.
Neste caso, a representação segue a mesma estrutura proposta pela codificação
\textit{bag-of-words}, entretanto, cada palavra é ponderada por um multiplicador
inversamente proporcional a sua frequência nos documentos.

% TODO: equação tf-idf

\textit{Bag-of-words} e TF-IDF são métodos de representação muito presentes na
literatura pela sua simplicidade de implementação e pelos benefícios
anteriormente descritos, em especial em conjunto com modelos como a SVM (TODO
link secao) que apresenta menos dificuldades de ser treinada em dados esparsos.
Entretanto, um componente fundamental da linguagem é perdido nesse processo, o
contexto em que se insere cada palavra.
Pela representação agrupar todos os tokens em um único vetos a ordem das
palavras no documento é perdida, o que em muitos casos pode corresponder na
inviabilidade de uma classificação precisa do mesmo.

% TODO: talvez mostrar exemplo em que a ordem faria diferença

As técnicas de representações densas que serão apresentadas a seguir além de
resolverem o obstáculo da dimensionalidade dos dados também viabilizam a
classificação por modelos que levem em conta o contexto de cada palavra.

\subsection{Word2Vec}

\textit{Word2Vec}~\cite{mikolov13} foi uma das primeiras técnicas de
representação densa de palavras amplamente adota pela indústria e academia.
Representações densas são aquelas em que cada palavra é transformada em um vetor
de números reais de baixa dimensionalidade, tipicamente dezenas ou poucas
centenas de dimensões.
\citet{mikolov13} mostraram que essa representação é capaz de capturar parte dos
sentidos semânticos e sintáticos das palavras, aproximando as que são sinônimas
ou exerçam a mesma função gramatical, atenuando assim o problema da disparidade
de frequência das palavras.

Esta técnica é um modelo constituído de uma rede neural de uma camada escondida.
Para o treinamento do mesmo são feitas janelas de um número arbitrário de
palavras que servirão como entrada do modelo.
Dessas janelas há duas variações: o \textit{continuous bag-of-words} em que
o modelo é treinado para prever a palavra central da janela de entrada a partir
das outras palavras da janela, ou o \textit{skipgram} que a partir da palavra
central da treina-se para prever o seu contexto.
As entradas e saídas do modelo são representadas pela codificação
\textit{one-hot} dos termos.
A quantidade de neurônios escolhido para a camada escondida da rede resultará na
dimensionalidade da representação.
A representação das palavras serão os pesos da camada de entrada do modelo
treinado.

% TODO diagramas

Apesar do \textit{Word2Vec} ser um modelo que precisa ser treinando, esse
treinamento é não-supervisionado dado que tanto as entradas quanto saídas do
modelo são obtidas diretamente dos documentos.
Desta forma, foi possível aplicar o algoritmo a grandes bases de dados
mineiradas da internet.
Esse alto volume de dados de treinamento foi essencial para que bons resultados
fossem alcançados.

Por ser a primeira representação densa de sucesso, o \textit{Word2Vec} foi
essencial para a aplicação de classificadores também baseados em redes neurais,
como os de \textit{Deep Learning} que obtiveram grande êxito em diversas
tarefas de processamento de linguagem natural.
Outras técnicas semelhantes e também amplamente adotadas na indústria foram
desenvolvidas neste mesmo período de tempo como o GloVe~\cite{pennington14} e
o FastText~\cite{bojanowski17}.

% TODO abrir matematica do W2V

\subsection{Representações por Redes Recorrentes}

O sucesso do \textit{Word2Vec}, baseado em redes neurais \textit{feed forward}
inspirou a experimentação de outros tipos de redes neurais, dentre as quais as
redes neurais recorrentes e suas variações obtiveram bons resultados na
representação de palavras.
Nesta seção serão descritos alguns destes algoritmos.

A estrutura chamada \textit{Enconder-Decoder} desenvolvida por \citet{cho14} se
tornou a base das principais representações por redes recorrentes.
Esta estrutura consiste em uma rede neural recorrente divida em duas etapas: a
codificação, na qual uma sequência de tamanho variável é representada em um
vetor; e a decodificação em que este mesmo vetor é utilizado para obter uma
outra sequência de tamanho variável.
O \textit{Encoder-Decoder} é um modelo de aprendizado de sequências de tamanho
variável a partir de outra sequência de tamanho váriavel como entrada, em que
os tamanhos não necessariamente são os mesmos, reflexo do problema inicial em
que o mesmo foi desenvolvido para atacar, a tradução de textos.

% TODO: diagrama (ver cho14)

Essa estrutura toda é treinada conjuntamente epode ser utilizada para gerar
sequências de saída a partir de entradas ou avaliar um par de entrada e saída a
partir da probabilidade $p_{\mathbf{\theta}}(\mathbf{y} \mid \mathbf{x})$
aprendida pelo treinamento, no qual $\mathbf{x}$ e $\mathbf{y}$ representam
respectivamente as sequências de entrada e saída e $\mathbf{\theta}$ o conjunto
de pesos do modelo treinado.

A etapa de codificação desse modelo pode ser vista como um resumo da sequência
de entrada em um vetor de tamanho fixo, assim sendo, o mesmo pode ser utilizado
para representações de documentos, ou palavras caso aplicada sequência de
tamanho unitário.
\citet{cho14} demostram brevemente em seu trabalho que a codificação é capaz de
capturar significados semânticos e sintático das palavras.

\subsubsection{ELMo}
% Deep contextualized word representations

ELMo~\cite{peters18}, sigla para \textit{Embeddings from Language Models}, em
português Representações para Modelos Linguísticos, se diferencia do modelo de
\citet{cho14} por levar em consideração o contexto de uma dada palavra para
obter sua representação, inspirado por \citet{peters17} e \citet{mccann17}.
Isto é, uma palavra que possua multiplos sentidos terá representações diferentes
dependendo da frase em que está inserida.
O modelo é composto de uma rede LSTM~\cite{hochreiter97} bi-direcional
multi-camada, treinada de maneira não supervisionada a partir do objetivo de
prever a palavra seguinte.

Estudos anteriores mostram que em modelos de redes recorrentes multi-camadas,
a melhor camada escolhida para representação das palavras depende da finalidade
em que se deseja aplica-la
~\cite{hashimoto16}~\cite{sogaard16}~\cite{belinkov17}~\cite{melamud16}, e que
em geral, camadas inferiores codificam melhor informação sintética e camadas
superiores a informação semântica.
A proposta de ELMo é para cada tarefa treinar uma combinação linear das
representações obtida por cada camada da rede.

% TODO: diagrama (ver http://jalammar.github.io/illustrated-bert/)

\citet{peters18} mostram que ELMo foi capaz de obter resultados melhores que o
estado da arte em diversas tarefas do processamento de linguagem natural.
Além de capturar informação sintética e semântica, ELMo se mostrou capaz de
desambiguar o contexto de palavras, problema até então não resolvido pelos
principais algoritmos de representação.

\subsection{BERT}
% bert - 2018 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

Outra técnica de representação bastante utilizada atualmente para representação
de palavras é a BERT~\cite{devlin18}, \textit{Bidirectional Transformers}, mas
antes de explica-la será apresentada abaixo a arquitetura Transformers em que a
mesma se baseia.

\subsubsection{Transformers}
% transformers - 2017 - Attention is all you need (boa explicacao dos conceitos)

Um dos principais problemas de redes neurais recorrentes é o "esquecimento".
O esquecimento é observado quando ao tentar prever uma palavra no final de uma
longa sequência se perde influência das primeiras palavras da mesma por causa da
sucessivas multiplicações dos pesos entre ambas.
Apesar do algotimo LSTM~\cite{hochreiter97}, \textit{Long-Short Term Memory},
que visa atenuar esse efeito criando um peso adicional, chamado de portão, que
controla o quanto os pesos de contexo são atualizados entre cada iteração de
palavras de uma sequência, o esquecimento continua sendo um fator limitante.

O mecanismo de atenção desenvolvido por \citet{bahdanau14} também tem como
objetivo diminuir o efeito do esquecimento.
Este consiste em um conjunto de pesos adicionais que multiplicados pelos pesos
de estados das iterações anteriores formam os pesos de estado da iteração
corrente, como também descrito em mais detalhes por \citet{luong15}.
A atenção se tornou uma das principais adições feitas a redes recorrentes
aplicadas a processamento de linguagem natural.

Inspirado nesse mecanismo, \citet{vaswani17} propuseram a arquitetura
Transformer para tradução de textos.
Essa arquitetura consiste em um sistema de \textit{Encoder-Decoder} desta vez
composto por redes neurais \textit{feed forward} multi camadas em que a
dependência temporal entre as palavras de um documento é atacada apenas pelo
sistema de \textit{self-attention}, ou, atenção própria.

Enquanto o mecanismo de atenção é composto se dá por um único conjunto de pesos,
a \textit{self-attention} é dividido em 3: os pesos de busca, de resposta e de
valor.
Cada palavra do vocabulário terá um de cada vetor associado a mesma que será
treinado em conjunto com o resto da arquitetura.
Para cada palavra do documento será feito um produto interno do seu vetor de
busca e os vetores de resposta das outras palavras da sentença, resultando em um
valor único de atenção entre cada par possível de palavras de uma sentença.
Após calcular esse valor será realizado uma operação de Softmax para que a norma
do vetor de atenção seja unitária.
Posteriormente se multiplica o vetor resultante pelo vetor de valor da palavra
de resposta, essa operação tem a finalidade de diminuir a importância de
palavras sem potencial discriminatório, como as \textit{stopwords}.
Assim são calculadas as entradas da rede \textit{feed forward}.

% TODO: figura exemplificando processo http://jalammar.github.io/illustrated-transformer/

% TODO: figura com o resultado http://jalammar.github.io/images/t/transformer_self-attention_visualization.png
% pode ser print do tensor2tensor

A etapa de codificação do Transformer é composta por uma cascata de unidades
formadas por uma camada de \textit{self-attention} seguida de uma camada de
rede \textit{feed forward}.
Diferentemente das redes recorrentes, o Transformer requer um tamanho fixo de
entrada, sendo um parâmetro a ser definido a depender da base de treinamento
escolhida.
Apesar das redes \textit{feed forward} não possuirem um conceito de sequência
assim como as redes recorrentes, essa arquitetura provou ser eficiente em
tarefas de processamento de linguagem natural ao mesmo tempo que possuem
treinamento significamente menos custosos que a mesma~\cite{vaswani17}.

\subsubsection{Arquitetura BERT}

O sucesso do Transformer em traduções inspirou sua aplicação em outras tarefas.
\citet{radford18} propuseram utilizar apenas a camada de decodificação do
Transformer junto com uma adaptação da sequência de entrada para adequar a
tarefa a ser realizada.
O modelo é pré-treinado com uma quantidade massiva de dados na predição da
palavra seguinte e posteriormente é feito um ajuste fino com uma base de dados
da tarefa desejada.
Esta nova arquitetura foi capaz propagar os ganhos de performance do Transformer
para toda gama de tarefas de NLP.

Entretanto, tanto o Transformer original quanto o de \citet{radford18} são
modelos unidirecionais, prevendo a palavra, ou sentença à direita a partir do
contexto à esquerda.
\citet{devlin18} defendem que por serem unidirecionais os modelos restringem a
capacidade dos mesmos, principalmente em tarefas que utilizam a sentença
inteira, sendo a análise de sentimento um exemplo.
Proporam então um modelo bidirecional praticamente idêntico ao Transformer de
\citet{radford18} sendo sua única modificação ter o mecanismo de atenção
considerando o contexto bidirecionalmente.

Assim como o ELMo, o BERT pode ser usado diretamente como classificador além de
representar palavras.
\citet{devlin18} mostram as diferenças entre as camadas do modelo quando
utilizados como representação, assim como nas arquiteturas apresentadas
anteriormente as diferentes camadas possuem aprendizado de características
distintas da lingua.

\section{Classificadores}

Nessa seção serão abordadas as diferentes estratégias para classificação de
sentimento.
As etapas descritas anteriormente lidam com a preparação dos documentos para
realização da classificação, apesar da complexidade dos métodos apresentados os
classificadores podem ser tão simples quanto contadores de palavras positivas e
negativas.
Podemos dividir-los em algoritmos baseados em dicionário e algoritmos de
aprendizado de máquina, suas caracteristicas serão descritas abaixo.

\subsection{Baseados em Dicionário} \label{sec:dictionary}

Uma das técnicas mais simples para classificação de sentimento é feita a partir
da elaboração de um dicionário composto por palavras que tenham conotação
positiva ou negativa.
Entre as maneiras de se montar um dicionario de sentimento é a partir de um
conjunto inicial de palavrase um dicionário de sinonimos e antonimos tal qual o
WordNet~\cite{miller90}.
Um exemplo é selecionar as palavras "bom" e "ruim" e montando uma base de
palavras recursivamente a partir dos seus sinonimos e antonimos.
Esse método é aplicado por \citet{hu04}.
% Liu 2012 sentiment analysis
% Mining and Summarizing Customer Reviews tem boas referencias no 2.2

\subsection{Lineares}
\subsection{Não Lineares}
\subsection{Deep Learning}
% diferença principal de deep p tradicional é: contexto
% inspiração: http://dataskeptic.com/blog/episodes/2019/natural-language-processing

% os modelos de representacao lstm tb sao de classificacao, o que fazer?
% https://ruder.io/nlp-imagenet/


% falar de diferentes dificultades de NLP e citar trabalhos relacionados no cap 3
